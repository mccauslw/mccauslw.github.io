---
title: "ECN 6578A, Économétrie des marchés financiers, Hiver 2019"
subtitle: "Cours 6"
author: "William McCausland"
date: "`r Sys.Date()`"
output: beamer_presentation
---

## Plan

1. Un modèle de volatilité stochastique
1. Inférence bayésienne : un peu de théorie
1. Inférence bayésienne : computation

## Un modèle de volatilité stochastique

* Un modèle de volatilité stochastique simple
$$ r_t = \mu + \sqrt{h_t} \epsilon_t, $$
$$ \log h_t = \alpha_0 + \alpha_1 \log h_{t-1} + v_t. $$
$$ \begin{bmatrix} \epsilon_t \\ v_t \end{bmatrix}
\sim N\left(0,\begin{bmatrix} 1 & 0 \\ 0 & \omega_v^{-2} \end{bmatrix} \right). $$

* La volatilité n'est pas une fonction déterministe de rendements passés.

* Évaluer la vraisemblance $f(r|\mu,\alpha_0,\alpha_1,\sigma_v^2)$ est difficile.

* On peut introduire une corrélation négative entre $\epsilon_t$ et $v_t$ pour capturer l'effet de levier.

## Motivation des méthodes Bayésiennes

* Analyse simple et élégante des modèles avec variables latentes
* En faisant des prévisions, on tient compte de l'incertitude sur les paramètres, les ordres ($p$ et $q$ par exemple) et les modèles.
* Il n'y a pas de recours au résultats asymptotiques.

## Éléments de l'analyse bayésienne

* Quantités pertinentes
    * $\theta$, un vecteur de paramètres inconnus
    * $y=(y_1,\ldots,y_T)$, un vecteur des variables observables
    * $y^\circ$, le vecteur observé
    
* Densités pertinentes
    * $f(y|\theta)$, densité conditionnnelles des données (modèle)
    * $L(\theta;y^\circ) = \log(f(y^\circ)|\theta)$, vraisemblance
    * $f(\theta)$, densité *a priori*
    * $f(\theta,y)$, densité conjointe
    * $f(\theta|y)$, densité *a posteriori*
    * $f(y)$, densité marginale des données
    * $f(y^\circ)$, vraisemblance marginale (un nombre)

## Inférence bayésienne

* Par la règle de Bayes,
$$ f(\theta|y^\circ) = \frac{f(\theta,y^\circ)}{f(y^\circ)} = \frac{f(\theta)f(y^\circ|\theta)}{f(y^\circ)}
\propto f(\theta)f(y^\circ|\theta). $$

* $f(\theta)$ représente notre incertitude sur $\theta$ avant l'observation de $y$.

* $f(\theta|y^\circ)$ resprésente notre incertitude sur $\theta$ après qu'observe $y=y^\circ$.

## Reprise et extension de l'exemple Bernoulli

* Si $y_i$ est Bernoulli avec probabilité $\theta$, $f(y|\theta) = \theta^{n_1} (1-\theta)^{n_0}$.

* Mettons qu'on choisit la loi *a priori* $\theta \sim \mathrm{Beta}(\alpha,\beta)$ sur $[0,1]$ :
$$ f(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}. $$
* La densité conjointe est
$$ f(\theta,y) = f(\theta)f(y|\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha + n_1 - 1} (1-\theta)^{\beta + n_0 - 1}. $$

* La loi *a posteriori* doit être $\theta \sim \mathrm{Beta}(\alpha + n_1, \beta + n_0)$.

* La vraisemblance marginale est $f(\theta,y)/f(\theta|y)$ :
$$ f(y) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(\alpha+n_1)\Gamma(\beta+n_0)}{\Gamma(\alpha + \beta + n)}. $$

## Graphique pour l'exemple Bernoulli

```{r bbpriorpost, echo=TRUE}
n0 = 200; n1 = 230; alpha=2; beta=2
x = seq(0, 1, by=0.002)
plot(x, dbeta(x, alpha+n1, beta+n0), type='l')
lines(x, dbeta(x, alpha, beta), col='red')
```

## Exemple gaussien I

* Considérez les modèle $y_t \sim \mathrm{iid}\, N(\mu, h^{-1})$.
* Le vecteur de paramètres est $\theta = (\mu,h)$.
* Le vecteur d'observables est $y = (y_1,\ldots,y_T)$.
* La densité des données est
$$ \begin{aligned} f(y|\theta) &= \prod_{t=1}^T \sqrt{\frac{h}{2\pi}} \exp\left[-\frac{h}{2}(y_t-\mu)^2\right] \\
&= \left(\frac{h}{2\pi}\right)^{T/2} \exp \left[-\frac{h}{2} \sum_{t=1}^T (y_t-\mu)^2 \right]. \end{aligned} $$

## Exemple gaussien II

* Mettons qu'on choisit une loi *a priori* où $h$ et $\mu$ sont indépendents,
avec
$$ \mu \sim N(\bar{\mu},\bar{\omega}^{-1}), \quad \bar{s}h \sim \chi^2(\bar{\nu}). $$
* La densité *a priori* est
$$ f(\theta) \propto \exp \left[-\frac{\bar{\omega}}{2}(\mu-\bar{\mu})^2\right]
\cdot h^{(\bar{\nu}-2)/2} \exp\left[-\frac{1}{2}\bar{s}^2 h\right]. $$
* La densité conjointe est
$$ f(\theta,y) \propto h^{(\bar{\nu}+T-2)/2} \exp\left[-\frac{\bar{\omega}}{2} (\mu-\bar{\mu})^2 - \frac{h}{2} \left( \bar{s}^2 + \sum_{t=1}^T (y_t - \mu)^2 \right) \right]. $$

## L'intégration et les objectifs de l'analyse bayésienne

* Plusieurs problèmes d'inférence bayésienne ont, comme solution, des intégrales.

* Exemple 1, estimation ponctuelle de $\theta_k$ sous perte quadratique:
$$ \hat{\theta}_k = E[\theta_k | y^\circ]. $$

* Exemple 2, quantification de l'incertitude sur $\theta_k$ :
$$ \mathrm{Var}[\theta|y^\circ] = E[(\theta_k - E[\theta_k|y^\circ])^2|y^\circ]. $$

* Exemple 3, densité prédictive (valeurs de $y_{T+1}$ sur une grille) :
$$ f(y_{T+1}|y^\circ) = E[f(y_{T+1}|\theta,y^\circ)|y^\circ]. $$

## Preuve de l'exemple 3

$$ \begin{aligned}
E[f(y_{T+1}&|y_1,\ldots,y_T,\theta)|y_1,\ldots,y_T] \\
&= \int f(y_{T+1}|y_1,\ldots,y_T,\theta) f(\theta|y_1,\ldots,y_T)\, d\theta \\
&= \int f(y_{T+1},\theta|y_1,\ldots,y_T)\, d\theta \\
&= f(y_{T+1}|y_1,\ldots,y_T)
\end{aligned} $$

## Méthodes pour trouver $E[g(\theta)|y^\circ]$

* Calcul analytique : élégant, exacte, presque toujours insoluble.

* Simulation Monte Carlo indépendante :
    * Si on peut simuler $\theta^m \sim \mathrm{iid}\, \theta|y^\circ$,
    $$ \frac{1}{M} \sum_{m=1}^M g(\theta^m) \rightarrow_p E[g(\theta)|y^\circ]. $$
    * Cependant, cette simulation est rarement faisable.

* Simulation Monte Carlo chaîne markovienne (MCMC) :
    * On choisit un processus markovien avec densité de transition $f(\theta^m|\theta^{m-1})$ telle que la loi *a posteriori* $\theta|y^\circ$ est la loi stationnaire du processus. C'est à dire :
    $$ \theta^{m-1} \sim f(\theta|y^\circ) \Rightarrow \theta^m \sim f(\theta|y^\circ). $$
    * Sous quelques conditions techniques, la loi de $\theta^m$ converge à la loi *a posteriori* et
$$ \frac{1}{M} \sum_{m=1}^M g(\theta^m) \rightarrow_p E[g(\theta)|y^\circ]. $$

## MCMC 1 : marche aléatoire metropolis-hastings

* Pour tirer $\theta^m|\theta^{m-1}$ :
    1. Tirer $\theta^* \sim N(\theta^{m-1}, \Sigma)$
    1. Calculer le ratio de Hastings :
    $$ H = \frac{f(\theta^*|y^\circ)}{f(\theta^{m-1}|y^\circ)}. $$
    1. Accepter $\theta^*$ avec probabilité $\min(1, H)$.
* Accepter $\theta^*$ veut dire $\theta^m = \theta^*$ ; si on n'accepte pas, $\theta^m = \theta^{m-1}$.
* On peut utiliser $f(\theta,y^\circ)$ au lieu de $f(\theta|y^\circ)$ parce que les constantes $f(y^\circ)$ s'annulent.
* La convergence tient pour n'importe quelle $\Sigma$, mais il y a des choix qui sont meilleures que d'autres.

## Initialisation

```{r da}
# Vraies valeurs des paramètres
vrai.mu = 6
vrai.h = 0.04
vrai.sigma = 1/sqrt(vrai.h)

# Données simulatées, statistiques suffisantes
n = 10
y = rnorm(n, vrai.mu, vrai.sigma)
y.bar = mean(y)
y2.bar = mean(y^2)

# Hyper-paramètres
mu.bar = 10
omega.bar = 0.01
nu.bar = 4
s2.bar = 0.01
```

## Fonctions pour calculer des densités

```{r dens}
# Densité a priori de mu
lnp.mu = function(mu) {
	lnp = dnorm(mu,mu.bar,1/sqrt(omega.bar),log=TRUE)
}

# Densité a priori de h
lnp.h = function(h) {
	lnp = log(s2.bar) + dchisq(h*s2.bar,nu.bar,log=TRUE)
}

# Densité des données
lnp.y..mu.h = function(mu, h) {
	lnp = (n/2)*log(h) - (n/2)*log(2*pi)
	lnp = lnp - 0.5*h*n * (y2.bar - 2*y.bar*mu + mu^2)	
}
```

## La densité conjointe

```{r joint}
# Densité a posteriori de mu and h, pas normalisée
lnp.mu.h..y = function(mu,h) {
	lnp = lnp.mu(mu) + lnp.h(h) + lnp.y..mu.h(mu,h)
}

# Fonction pour faire un graphique de la densité a posteriori
do.plot = function() {
	mu = seq(0,12,by=0.01)
	h = seq(0,0.12,by=0.0001)
	p = outer(mu,h,FUN=lnp.mu.h..y)
	levels = c(-46,-47,-48,-49,-50,-51,-52,-53,-54,-55,-56)
	contour(mu,h,p,xlab='mu',ylab='h',levels=levels)
	points(vrai.mu, vrai.h, col='red')	
}
```

## Graphique de la densité conjointe

```{r plot}
do.plot()
```

## Code pour l'algorithme Metropolis Hastings

```{r MH}
Metro.sim = function(M) {
  mu = vector('numeric',M); h = vector('numeric',M)
  mu[1] = 0; h[1] = 1
  lnp = lnp.mu.h..y(mu[1], h[1])
  for( m in seq(2,M) ) {
    h.et=rnorm(1,h[m-1],0.05); mu.et=rnorm(1,mu[m-1],2.0)
    if( h.et > 0.0 ) {
      lnp.et = lnp.mu.h..y(mu.et, h.et)
    } else lnp.et = -Inf
    H = exp(lnp.et - lnp)
    if( runif(1) < H ) {
      h[m] = h.et; mu[m] = mu.et; lnp = lnp.et
    } else {
      h[m] = h[m-1]; mu[m] = mu[m-1]
    }
  }
  list(mu=mu, h=h)
}
```

## Trace de $h$

```{r trace_h}
sim.MH = Metro.sim(1000)
plot(sim.MH$h)
```

## Trace de $\mu$

```{r trace_mu}
plot(sim.MH$mu)
```

## Histogramme de $h$

```{r hist_h}
hist(sim.MH$h[101:1000], 20)
```

## Histogramme de $\mu$

```{r hist_mu}
hist(sim.MH$mu[101:1000], 20)
```

## MCMC 2 : échantillonage de gibbs pour le modèle gaussien

* Considérez la densité de transition $f(\theta^m|\theta^{m-1})$ définie par
    1. $\mu^m \sim \mu|y=y^\circ, h=h^{m-1}$.
    1. $h^m \sim h|y=y^\circ, \mu=\mu^m$.
* Une preuve que $\theta|y^\circ$ est la loi stationnaire de cette loi de transition :
    * Mettons que la loi de $\theta^{m-1} = (\mu^{m-1},h^{m-1})$ est la loi *a posteriori* $\theta|y=y^\circ$.
    * Alors la loi de $h^{m-1}$ est la loi $h|y=y^\circ$.
    * Après l'étape 1, la loi de $(\mu^m,h^{m-1})$ est la loi *a posteriori*.
    * Alors la loi de $\mu^m$ est la loi $\mu|y=y^\circ$.
    * Après l'étape 2, la loi de $\theta^m = (\mu^m,h^m)$ est la loi *a posteriori*.
* L'idée se généralise (diviser un problème en parties soluables)

## Dérivation de loi *a posteriori* conditionnelle de $\mu|y=y^\circ,h$

* On peut écrire
$$ \begin{aligned} -\frac{\bar{\omega}}{2} &(\mu-\bar{\mu})^2 - \frac{h}{2} \sum_{t=1}^T (y_t - \mu)^2 \\ &= -\frac{\bar{\omega}}{2} (\mu-\bar{\mu})^2 - \frac{h}{2} \left[ \sum_{t=1}^T (y_t - \bar{y})^2 + T(\mu-\bar{y})^2 \right] \\ &= c_1 - \frac{\bar{\omega}}{2} (\mu-\bar{\mu})^2 - \frac{hT}{2} (\mu-\bar{y})^2 \\
&= c_2 - \frac{\bar{\omega} + hT}{2} [\mu - (\bar{\omega} \bar{\mu} + hT  \bar{y})/(\bar{\omega} + hT)]^2. \end{aligned} $$

* Dernière étape : [\textcolor{blue}{complétion du carré}](https://en.wikipedia.org/wiki/Completing_the_square)

* Alors $f(\mu|y,h) \propto \exp\left[-\frac{\bar{\bar{\omega}}}{2} (\mu-\bar{\bar{\mu}})^2\right]$, où
    * $\bar{\bar{\mu}} = \frac{\bar{\omega} \bar{\mu} + hT \bar{y}}{\bar{\omega} + hT}$,
    * $\bar{\bar{\omega}} = \bar{\omega} + hT$.

## MCMC 3 : échantillonage de gibbs pour le modèle SV

Faire $M$ fois :

1. Tirer $\alpha_0$ et $\alpha_1$ de la loi *a posteriori* conditionnelle $(\alpha_0,\alpha_1)|y,\omega_v,h_1,\ldots,h_T$.
1. Tirer $\omega_v$ de la loi *a posterior* conditionnelle $\omega_v|y,\alpha_0,\alpha_1,h_1,\ldots,h_T$
1. Tirer $h_1,\ldots,h_T$ de la loi *a posteriori* conditionnelle
$h_1,\ldots,h_T|y,\alpha_0,\alpha_1,\omega_v$.

## Questions théoriques

1. Trouvez la loi *a posteriori* quand les observations sont iid $\mathrm{Poisson}(\lambda)$ et la loi *a priori* de $\lambda$ est la loi $\mathrm{Gamma(\alpha, \beta)}$.

1. Trouvez la loi *a posteriori* conditionnelle de $h$ dans le modèle gaussien.

1. Dans le modèle de volatilité stochastique, montrez que
$$ \begin{aligned} f(y_{T+1}|y_1,\ldots,y_T) = E[&f(\log h_{T+1}|\log h_T, \theta, y_1,\ldots,y_T) \cdot \\ &f(y_{T+1}|\log h_{T+1}, \log h_T, \theta, y_1,\ldots,y_T)], \end{aligned} $$
où l'espérance est par rapport à la loi conditionnelle de $(\theta,h_T)$ sachant $y_1,\ldots,y_T$. Écrivez les densités $f(\log h_{T+1}|\log h_T, \theta, y_1,\ldots,y_T)$ et $f(y_{T+1}|\log h_{T+1}, \log h_T, \theta, y_1,\ldots,y_T)$ en utilisant les équations d'état et d'observation. Comment peut-on approximer la densité prédictive $f(y_{T+1}|y_1,\ldots,y_T)$ sur une grille à partir d'un échantillon de la loi de $\theta,\log h_T|y_1,\ldots,y_T$?
